{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Identification Project\n",
    "#### Dov Greenwood, Shalaka Kulkarni, Keaton Mueller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.ensemble_model import Ensemble\n",
    "from models.compression_model import CompressionModel\n",
    "from models.character_ngram_svm import CNGM\n",
    "from models.word_frequency_model import BOW\n",
    "\n",
    "from utils.data_reader import read_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_reader import read_data\n",
    "from random import shuffle\n",
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "\n",
    "def segment(string, chunk_size):\n",
    "    '''\n",
    "    Segments `string` into a list of strings of length `chunk_size`\n",
    "    '''\n",
    "    return [string[i:i + chunk_size] for i in range(0, len(string), chunk_size)]\n",
    "\n",
    "STORED_DATA = None\n",
    "\n",
    "def prep_data(authors, test_pct=0, chunk_size=0):\n",
    "    '''\n",
    "    Prep data to be used in models.\n",
    "    \n",
    "    `authors` is a list of authors to prep data for.\n",
    "    \n",
    "    `test_pct` is a number between 0 and 1 indicating how much of the\n",
    "    additional poetry per author should be designated as the test data.\n",
    "    The Iliad will always be in the test data.\n",
    "    \n",
    "    `chunk_size` is an integer indicating how many characters to segment\n",
    "    each piece into. 0 indicates no segmenting\n",
    "    \n",
    "    Returns a dict of the form:\n",
    "    {\n",
    "        'train': {\n",
    "            author_name: [text, text, ...],\n",
    "            author_name: [text, text, ...],\n",
    "            ...\n",
    "        }\n",
    "        'test': [\n",
    "            (text, author_name),\n",
    "            (text, author_name),\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    '''\n",
    "    global STORED_DATA\n",
    "    # read in data if this is the first time calling this function\n",
    "    if STORED_DATA is None:\n",
    "        STORED_DATA = read_data()\n",
    "        \n",
    "    # extract the authors of interest\n",
    "    raw_data = { auth: deepcopy(texts) for auth, texts in STORED_DATA.items() if auth in authors }\n",
    "        \n",
    "    data = {\n",
    "        'train': {},\n",
    "        'test': []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # segment data if chunk_size > 0\n",
    "    if chunk_size > 0:\n",
    "        for auth in raw_data:\n",
    "            raw_data[auth]['iliad'] = segment(raw_data[auth]['iliad'], chunk_size)\n",
    "            raw_data[auth]['poetry'] = segment('\\n'.join(raw_data[auth]['poetry']), chunk_size)\n",
    "    \n",
    "    # iliad always gets added to test data\n",
    "    for auth in raw_data:\n",
    "        iliad = raw_data[auth]['iliad']\n",
    "        if type(iliad) == list:\n",
    "            data['test'].extend([(text, auth) for text in iliad])\n",
    "        elif type(iliad) == str:\n",
    "            data['test'].append((iliad, auth))\n",
    "    \n",
    "    # split data into training and test\n",
    "    for auth in raw_data:\n",
    "        auth_data = raw_data[auth]['poetry']\n",
    "        # shuffle so different texts are sorted into training and test (optional)\n",
    "        shuffle(auth_data)\n",
    "        \n",
    "        idx = ceil(len(auth_data) * test_pct)\n",
    "        test_set = auth_data[:idx]\n",
    "        training_set = auth_data[idx:]\n",
    "        \n",
    "        data['train'][auth] = training_set\n",
    "        data['test'].extend([(text, auth) for text in test_set])\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prf(matrix):\n",
    "    '''\n",
    "    Calculates precision, recall, and F1 score for the given\n",
    "    confusion matrix. MUST be a simple 2x2 confusion matrix.\n",
    "    \n",
    "    Handles division by zero cases based on the following\n",
    "    methodology: https://github.com/dice-group/gerbil/wiki/Precision,-Recall-and-F1-measure\n",
    "    \n",
    "    returns (precision, recall, F1 score) tuple\n",
    "    '''\n",
    "    if matrix['tp'] == 0:\n",
    "        if matrix['fp'] == 0 and matrix['fn'] == 0:\n",
    "            return 1.0, 1.0, 1.0\n",
    "        if matrix['fp'] == 0 or matrix['fn'] == 0:\n",
    "            return 0, 0, 0\n",
    "    \n",
    "    precision = matrix['tp'] / (matrix['tp'] + matrix['fp'])\n",
    "    recall = matrix['tp'] / (matrix['tp'] + matrix['fn'])\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "def score_model(model, data):\n",
    "    '''\n",
    "    Function to test the model on the given data.\n",
    "    \n",
    "    `data` should be in the form of the output of \n",
    "    the `prep_data` function.\n",
    "    \n",
    "    return (micro_f1, macro_f1, confusion_matrix) tuple\n",
    "    '''\n",
    "    training_data = data['train']\n",
    "    test_data = data['test']\n",
    "\n",
    "    model.train(training_data)\n",
    "    \n",
    "    # confusion matrix to store results used for calculating precision and recall\n",
    "    # confusion_matrix[gold_label][model_label] = number of occurrences\n",
    "    confusion_matrix = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    for text, gold_label in test_data:\n",
    "        model_label = model.identify(text)[0][1]        \n",
    "        confusion_matrix[gold_label][model_label] += 1\n",
    "    \n",
    "    # compute individual class matrices\n",
    "    class_matrix = {}\n",
    "    num_results = sum([sum(confusion_matrix[auth].values()) for auth in confusion_matrix])\n",
    "    \n",
    "    for auth in confusion_matrix:\n",
    "        class_matrix[auth] = {\n",
    "            'tp': confusion_matrix[auth][auth],\n",
    "            'fp': sum([confusion_matrix[i][auth] for i in confusion_matrix if i != auth]),\n",
    "            'fn': sum(confusion_matrix[auth].values()) - confusion_matrix[auth][auth]\n",
    "        }\n",
    "        class_matrix[auth]['tn'] = num_results - sum(class_matrix[auth].values())\n",
    "        \n",
    "    # compute a pooled matrix\n",
    "    pooled_matrix = {\n",
    "        'tp': sum([class_matrix[auth]['tp'] for auth in class_matrix]),\n",
    "        'fp': sum([class_matrix[auth]['fp'] for auth in class_matrix]),\n",
    "        'tn': sum([class_matrix[auth]['tn'] for auth in class_matrix]),\n",
    "        'fn': sum([class_matrix[auth]['fn'] for auth in class_matrix])\n",
    "    }\n",
    "    \n",
    "    # micro precision, recall, and F1\n",
    "    micro_precision, micro_recall, micro_f1 = calculate_prf(pooled_matrix)\n",
    "\n",
    "    # macro precision, recall, and F1\n",
    "    class_precision = {}\n",
    "    class_recall = {}\n",
    "    \n",
    "    for auth in class_matrix:\n",
    "        p, r, _ = calculate_prf(class_matrix[auth])\n",
    "        class_precision[auth] = p\n",
    "        class_recall[auth] = r\n",
    "    \n",
    "    macro_precision = sum(class_precision.values()) / len(class_precision)\n",
    "    macro_recall = sum(class_recall.values()) / len(class_recall)\n",
    "    macro_f1 = (2 * macro_precision * macro_recall) / (macro_precision + macro_recall)\n",
    "    \n",
    "    # return F1 scores and confusion matrix\n",
    "    return micro_f1, macro_f1, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: add Cowper once the data is done\n",
    "authors = ['Pope', 'Dryden', 'Chapman']\n",
    "\n",
    "def test_model(model):\n",
    "    '''\n",
    "    Thoroughly test the given model.\n",
    "    \n",
    "    Model is evaluated on different chunk sizes, different\n",
    "    training/test data amounts, and with different\n",
    "    groupings of authors.\n",
    "    \n",
    "    Return a dictionary containing all of the results\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    # do tests on several different chunk sizes\n",
    "    # (add to the list of chunk sizes you want -- it makes things take very long!!)\n",
    "    for chunk_size in [0]:\n",
    "        # first perform a test on all authors\n",
    "        results[f'{chunk_size}|all'] = score_model(model, prep_data(authors, chunk_size=chunk_size))\n",
    "\n",
    "        # then do every pair-wise comparison\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                auth_1 = authors[i]\n",
    "                auth_2 = authors[j]\n",
    "                results[f'{chunk_size}|{auth_1} + {auth_2}'] = score_model(model, prep_data([auth_1, auth_2], chunk_size=chunk_size))\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_f1(model, train_data, folds=10):\n",
    "    \"\"\"\n",
    "    Tests the given model by performing k-fold cross validation on the training data.\n",
    "    \n",
    "    model should be a model class, inherting AbstractModel.\n",
    "    train_data should be the training data exactly as it should be prepared for the model classes.\n",
    "    \n",
    "    Returns the average Micro- and Macro-F1 score of the model as a numpy array.\n",
    "    \"\"\"\n",
    "    train_data = {auth : '\\n\\n'.join(train_data[auth]) for auth in train_data}\n",
    "    folded = {auth : [] for auth in train_data}\n",
    "    for auth in train_data:\n",
    "        lines = train_data[auth].split('\\n')\n",
    "        n_lines = len(lines)\n",
    "        step = n_lines // folds\n",
    "        for i in range(0, n_lines, step):\n",
    "            folded[auth].append('\\n'.join(lines[i : i+step]))\n",
    "    \n",
    "    aggregate = np.zeros((folds, 2))\n",
    "    for i in range(folds):\n",
    "        fold_train = {auth : (folded[auth][:i] + folded[auth][i+1:]) for auth in folded}\n",
    "        fold_test = [(folded[auth][i], auth) for auth in folded]\n",
    "        test_vals = score_model(model, {'train': fold_train, 'test': fold_test})\n",
    "        aggregate[i] = [test_vals[0], test_vals[1]]\n",
    "        ###print(aggregate[i])\n",
    "    return np.mean(aggregate, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Individual Models using `k_fold_f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts/Dryden/Dryden_train1.txt') as text: d = text.read()\n",
    "with open('texts/Pope/Pope_train1.txt') as text: p = text.read()\n",
    "with open('texts/Chapman/Chapman_train1.txt', encoding='UTF-8') as text: c = text.read()\n",
    "with open('texts/Chapman/Chapman_train2.txt', encoding='UTF-8') as text: c += text.read()\n",
    "with open('texts/Chapman/Chapman_train3.txt', encoding='UTF-8') as text: c += text.read()\n",
    "with open('texts/Chapman/Chapman_train4.txt', encoding='UTF-8') as text: c += text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cngm = CNGM(extended_alphabet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cngm_eval = k_fold_f1(cngm, {'p':[p], 'c':[c], 'd':[d]}, folds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98333333, 0.97857143])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cngm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = CompressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_eval = k_fold_f1(comp, {'p':[p], 'c':[c], 'd':[d]}, folds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83333333, 0.78690476])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_eval = k_fold_f1(bow, {'p':[p], 'c':[c], 'd':[d]}, folds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91666667, 0.89404762])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Weights, Initialize Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom = cngm_eval[0] + comp_eval[0] + bow_eval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = (bow_eval[0]/denom, comp_eval[0]/denom, cngm_eval[0]/denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3353658536585366, 0.3048780487804878, 0.3597560975609756)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensm = Ensemble(weights=wts, CNGM_specs=(2, 0, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Ensemble Model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensm_eval = k_fold_f1(ensm, {'p':[p], 'c':[c], 'd':[d]}, folds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98333333, 0.97857143])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensm_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Ensemble Model on Iliad test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Compression Model using `test_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0|all': (1.0,\n",
       "  1.0,\n",
       "  defaultdict(<function __main__.score_model.<locals>.<lambda>()>,\n",
       "              {'Chapman': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Chapman': 1, 'Dryden': 0, 'Pope': 0}),\n",
       "               'Dryden': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Dryden': 1, 'Chapman': 0, 'Pope': 0}),\n",
       "               'Pope': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Pope': 1, 'Chapman': 0, 'Dryden': 0})})),\n",
       " '0|Pope + Dryden': (1.0,\n",
       "  1.0,\n",
       "  defaultdict(<function __main__.score_model.<locals>.<lambda>()>,\n",
       "              {'Dryden': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Dryden': 1, 'Pope': 0}),\n",
       "               'Pope': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Pope': 1, 'Dryden': 0})})),\n",
       " '0|Pope + Chapman': (1.0,\n",
       "  1.0,\n",
       "  defaultdict(<function __main__.score_model.<locals>.<lambda>()>,\n",
       "              {'Chapman': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Chapman': 1, 'Pope': 0}),\n",
       "               'Pope': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Pope': 1, 'Chapman': 0})})),\n",
       " '0|Dryden + Chapman': (1.0,\n",
       "  1.0,\n",
       "  defaultdict(<function __main__.score_model.<locals>.<lambda>()>,\n",
       "              {'Chapman': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Chapman': 1, 'Dryden': 0}),\n",
       "               'Dryden': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Dryden': 1, 'Chapman': 0})}))}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.compression_model import CompressionModel\n",
    "\n",
    "test_model(CompressionModel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
