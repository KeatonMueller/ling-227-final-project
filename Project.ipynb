{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incredible-nomination",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compliant-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_reader import read_data\n",
    "from random import shuffle\n",
    "from math import ceil\n",
    "\n",
    "def segment(string, chunk_size):\n",
    "    '''\n",
    "    Segments `string` into a list of strings of length `chunk_size`\n",
    "    '''\n",
    "    return [string[i:i + chunk_size] for i in range(0, len(string), chunk_size)]\n",
    "\n",
    "def prep_data(authors, test_pct=0, chunk_size=0):\n",
    "    '''\n",
    "    Prep data to be used in models.\n",
    "    \n",
    "    `authors` is a list of authors to prep data for.\n",
    "    \n",
    "    `test_pct` is a number between 0 and 1 indicating how much of the\n",
    "    additional poetry per author should be designated as the test data.\n",
    "    The Iliad will always be in the test data.\n",
    "    \n",
    "    `chunk_size` is an integer indicating how many characters to segment\n",
    "    each piece into. 0 indicates no segmenting\n",
    "    \n",
    "    Returns a dict of the form:\n",
    "    {\n",
    "        'train': {\n",
    "            author_name: [text, text, ...],\n",
    "            author_name: [text, text, ...],\n",
    "            ...\n",
    "        }\n",
    "        'test': [\n",
    "            (text, author_name),\n",
    "            (text, author_name),\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    '''\n",
    "    data = {\n",
    "        'train': {},\n",
    "        'test': []\n",
    "    }\n",
    "    raw_data = read_data(authors)\n",
    "    \n",
    "    # segment data if chunk_size > 0\n",
    "    if chunk_size > 0:\n",
    "        for auth in raw_data:\n",
    "            raw_data[auth]['iliad'] = segment(raw_data[auth]['iliad'], chunk_size)\n",
    "            raw_data[auth]['poetry'] = segment('\\n'.join(raw_data[auth]['poetry']), chunk_size)\n",
    "    \n",
    "    # iliad always gets added to test data\n",
    "    for auth in raw_data:\n",
    "        iliad = raw_data[auth]['iliad']\n",
    "        if type(iliad) == list:\n",
    "            data['test'].extend([(text, auth) for text in iliad])\n",
    "        elif type(iliad) == str:\n",
    "            data['test'].append((iliad, auth))\n",
    "    \n",
    "    # split data into training and test\n",
    "    for auth in raw_data:\n",
    "        auth_data = raw_data[auth]['poetry']\n",
    "        # shuffle so different texts are sorted into training and test (optional)\n",
    "        shuffle(auth_data)\n",
    "        \n",
    "        idx = ceil(len(auth_data) * test_pct)\n",
    "        test_set = auth_data[:idx]\n",
    "        training_set = auth_data[idx:]\n",
    "        \n",
    "        data['train'][auth] = training_set\n",
    "        data['test'].extend([(text, auth) for text in test_set])\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-tragedy",
   "metadata": {},
   "source": [
    "# Evaluating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greater-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from utils.data_reader import read_data\n",
    "\n",
    "def calculate_prf(matrix):\n",
    "    '''\n",
    "    Calculates precision, recall, and F1 score for the given\n",
    "    confusion matrix. MUST be a simple 2x2 confusion matrix.\n",
    "    \n",
    "    Handles division by zero cases based on the following\n",
    "    methodology: https://github.com/dice-group/gerbil/wiki/Precision,-Recall-and-F1-measure\n",
    "    \n",
    "    returns (precision, recall, F1 score) tuple\n",
    "    '''\n",
    "    if matrix['tp'] == 0:\n",
    "        if matrix['fp'] == 0 and matrix['fn'] == 0:\n",
    "            return 1.0, 1.0, 1.0\n",
    "        if matrix['fp'] == 0 or matrix['fn'] == 0:\n",
    "            return 0, 0, 0\n",
    "    \n",
    "    precision = matrix['tp'] / (matrix['tp'] + matrix['fp'])\n",
    "    recall = matrix['tp'] / (matrix['tp'] + matrix['fn'])\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "def score_model(model, data):\n",
    "    '''\n",
    "    Function to test the model on the given data.\n",
    "    \n",
    "    `data` should be in the form of the output of \n",
    "    the `prep_data` function.\n",
    "    \n",
    "    return (micro_f1, macro_f1, confusion_matrix) tuple\n",
    "    '''\n",
    "    training_data = data['train']\n",
    "    test_data = data['test']\n",
    "\n",
    "    model.train(training_data)\n",
    "    \n",
    "    # confusion matrix to store results used for calculating precision and recall\n",
    "    # confusion_matrix[gold_label][model_label] = number of occurrences\n",
    "    confusion_matrix = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    for text, gold_label in test_data:\n",
    "        model_label = model.identify(text)[0][1]        \n",
    "        confusion_matrix[gold_label][model_label] += 1\n",
    "    \n",
    "    # compute individual class matrices\n",
    "    class_matrix = {}\n",
    "    num_results = sum([sum(confusion_matrix[auth].values()) for auth in confusion_matrix])\n",
    "    \n",
    "    for auth in confusion_matrix:\n",
    "        class_matrix[auth] = {\n",
    "            'tp': confusion_matrix[auth][auth],\n",
    "            'fp': sum([confusion_matrix[i][auth] for i in confusion_matrix if i != auth]),\n",
    "            'fn': sum(confusion_matrix[auth].values()) - confusion_matrix[auth][auth]\n",
    "        }\n",
    "        class_matrix[auth]['tn'] = num_results - sum(class_matrix[auth].values())\n",
    "        \n",
    "    # compute a pooled matrix\n",
    "    pooled_matrix = {\n",
    "        'tp': sum([class_matrix[auth]['tp'] for auth in class_matrix]),\n",
    "        'fp': sum([class_matrix[auth]['fp'] for auth in class_matrix]),\n",
    "        'tn': sum([class_matrix[auth]['tn'] for auth in class_matrix]),\n",
    "        'fn': sum([class_matrix[auth]['fn'] for auth in class_matrix])\n",
    "    }\n",
    "    \n",
    "    # micro precision, recall, and F1\n",
    "    micro_precision, micro_recall, micro_f1 = calculate_prf(pooled_matrix)\n",
    "\n",
    "    # macro precision, recall, and F1\n",
    "    class_precision = {}\n",
    "    class_recall = {}\n",
    "    \n",
    "    for auth in class_matrix:\n",
    "        p, r, _ = calculate_prf(class_matrix[auth])\n",
    "        class_precision[auth] = p\n",
    "        class_recall[auth] = r\n",
    "    \n",
    "    macro_precision = sum(class_precision.values()) / len(class_precision)\n",
    "    macro_recall = sum(class_recall.values()) / len(class_recall)\n",
    "    macro_f1 = (2 * macro_precision * macro_recall) / (macro_precision + macro_recall)\n",
    "    \n",
    "    # return F1 scores and confusion matrix\n",
    "    return micro_f1, macro_f1, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "former-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: add Cowper once the data is done\n",
    "authors = ['Pope', 'Dryden', 'Chapman']\n",
    "\n",
    "def test_model(model):\n",
    "    '''\n",
    "    Thoroughly test the given model.\n",
    "    \n",
    "    Model is evaluated on different chunk sizes, different\n",
    "    training/test data amounts, and with different\n",
    "    groupings of authors.\n",
    "    \n",
    "    Return a dictionary containing all of the results\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    # do tests on several different chunk sizes\n",
    "    # (add to the list of chunk sizes you want -- it makes things take very long!!)\n",
    "    for chunk_size in [0]:\n",
    "        # first perform a test on all authors\n",
    "        results[f'{chunk_size}|all'] = score_model(model, prep_data(authors, chunk_size=chunk_size))\n",
    "\n",
    "        # then do every pair-wise comparison\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                auth_1 = authors[i]\n",
    "                auth_2 = authors[j]\n",
    "                results[f'{chunk_size}|{auth_1} + {auth_2}'] = score_model(model, prep_data([auth_1, auth_2], chunk_size=chunk_size))\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "another-chile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0|all': (1.0,\n",
       "  1.0,\n",
       "  defaultdict(<function __main__.score_model.<locals>.<lambda>()>,\n",
       "              {'Chapman': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Chapman': 1, 'Dryden': 0, 'Pope': 0}),\n",
       "               'Dryden': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Dryden': 1, 'Chapman': 0, 'Pope': 0}),\n",
       "               'Pope': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Pope': 1, 'Chapman': 0, 'Dryden': 0})})),\n",
       " '0|Pope + Dryden': (1.0,\n",
       "  1.0,\n",
       "  defaultdict(<function __main__.score_model.<locals>.<lambda>()>,\n",
       "              {'Dryden': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Dryden': 1, 'Pope': 0}),\n",
       "               'Pope': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Pope': 1, 'Dryden': 0})})),\n",
       " '0|Pope + Chapman': (1.0,\n",
       "  1.0,\n",
       "  defaultdict(<function __main__.score_model.<locals>.<lambda>()>,\n",
       "              {'Chapman': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Chapman': 1, 'Pope': 0}),\n",
       "               'Pope': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Pope': 1, 'Chapman': 0})})),\n",
       " '0|Dryden + Chapman': (1.0,\n",
       "  1.0,\n",
       "  defaultdict(<function __main__.score_model.<locals>.<lambda>()>,\n",
       "              {'Chapman': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Chapman': 1, 'Dryden': 0}),\n",
       "               'Dryden': defaultdict(<function __main__.score_model.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                           {'Dryden': 1, 'Chapman': 0})}))}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.compression_model import CompressionModel\n",
    "\n",
    "test_model(CompressionModel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
